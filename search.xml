<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[scrapy分布式爬虫打造搜索引擎（三）—— scrapy爬虫实战]]></title>
    <url>%2F2019%2F05%2F03%2Fscrapy%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E6%89%93%E9%80%A0%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%88%E4%B8%89%EF%BC%89%E2%80%94%E2%80%94-scrapy%E7%88%AC%E8%99%AB%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[爬虫首战之爬取文章-伯乐在线上的文章及封面图 制定爬取策略 通过观察文章页面的网页结构，我们点开最新文章标签，发现它提供了所有文章的url，这样就不需要通过深度优先或广度优先的方法去爬取。 因为以后文章数会增加，因此页数也会相应地增加，为了我们地爬取代码能自适应增加内容，我们可以通过下一页来获取每页上的文章内容，而不是通过某个具体的页数来爬取。 爬取提示 在pycharm中建立项目，我用的是anaconda的python解释器，通过anaconda prompt进入项目目录，scrapy startproject [目录名]即在项目目录下建立了scrapy框架。 在项目目录下建立main.py文件方便以后调试爬虫 修改settings.py下ROBOTSTXT_OBEY = False(否则scrapy会默认过滤掉不符合ROBOTS协议的url) 这里有个坑，scrapy crawl [爬虫文件名]必须cd到有scrapy.cfg那个目录下才能执行，否则会报错Unknown command: crawl 参见官网:命令行工具(Command line tools) pycharm下在根目录下还是会出现Unknown command: crawl ,解决办法参见该篇博客中的第二种方法,即把scrapy.cfg移到项目根目录外 scrapy爬取的是未加载js之前的html 调试技巧，在命令行下运行scrapy shell [url] scrapy提供了xpath和css选择器两种方法提前html中的相应字段,也可以使用scrapy.loader中的itemloader scrapy提供了exporter模块可以很方便地将item导出成各种格式的文件，如xml，json，csv等 使用 from twisted.enterprise import adbapi可以将mysql的插入操作变成异步操作，因为常规使用pymysql的cursor的话是跟不上爬虫的解析速度的 爬取过程 在anaconda prompt中cd到保存爬虫项目的根目录，输入scrapy startproject article_spider，即在当前目录下生成了一个scrapy项目的框架。 根据提示，再cd到刚刚生成的项目目录下，输入scrapy genspider jobbole http://blog.jobbole.com/all-posts/后，scrapy即帮我们在article_spider\spiders\文件夹下生成了一个jobbole.py的文件，这个文件用于编写我们的后续爬虫逻辑。 为了便于调试，我们首先在项目目录下建立一个main.py文件，内容如下： 修改settings.py中的ROBOTSTXT_OBEY为False，否则在后续爬取过程中它会自动过滤掉一些不符合这个规则的url 接着在jobbole.py下编写我们的爬虫逻辑，主要分为两大部分： 获取文章列表页中每篇文章的url并交给scrapy进行下载后并进行解析 获取下一页的url并交给scrapy进行下载，下载完成后交给parse 接着继续在jobbole.py下编写具体解析一篇文章的函数parse_detail，大致工作分为如下几部分: 通过css选择器提取字段，如： 对于某些内容，我们只能提取到它的字符串，而实际上我们需要的只是其中的那个数字，这时我们就需要使用正则表达式去匹配我们需要提前的数字，如： 对提取到的字段做一些规范性处理，如： 在提取完所有的字段并都对它们进行了规范性处理后，我们在items.py中建立一个class来生成这些字段，如：并在jobbole.py中实例化这个class，并对每个字段赋上面我们规范化处理后的值，如： 生成item之后，我们考虑将这些数据持久化，保存到关系型数据库mysql中，这可以通过scrapy提供的pipline轻松完成，如： 爬取结果 将文章爬取的封面图保存到了article_spider\images\full下，如： 将爬取到的文章内容保存到了mysql关系型数据库中，如：]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>spider</tag>
        <tag>python</tag>
        <tag>scrapy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy分布式爬虫打造搜索引擎（二）——基础知识]]></title>
    <url>%2F2019%2F05%2F02%2Fscrapy%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E6%89%93%E9%80%A0%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94%E2%80%94%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%2F</url>
    <content type="text"><![CDATA[有关爬虫的一些基本知识 技术选型scrapy vs requests+beautifulsoup requests和beautifulsoup是库，scrapy是框架 scrapy框架中可以加入requests和beautifulsoup scrapy基于twisted(一个异步I/O框架)，性能是最大的优势 scrapy方便扩展，提供了很多内置功能 scrapy内置的css和xpath selector非常方便，beautifulsoup最大的缺点就是慢（因为xpath selector基于C，而beautifulsoup基于python） 网页分类常见类型的服务 静态网页 动态网页 webservice(restapi) //实际也是动态网页的一种，通过ajax方式与后台进行交互 静态网页与动态网页最大的区别在于它不需要与数据库进行交互 爬虫能做什么爬虫作用 搜索引擎 —— 百度、google、垂直领域搜索引擎 推荐引擎 —— 今日头条(根据用户浏览习惯爬取相关内容推送给用户) 机器学习的数据样本 数据分析，如金融数据分析，舆情分析等 正则表达式有关正则表达式的学习实际上是一个不小的工程，这里考虑后面新开一篇博客记录正则表达式的学习。这里仅作简单介绍。 正则表达式介绍有什么用？ 用正则表达式过滤通过爬虫爬取的html，从而获取到我们想要的内容_目录_ 特殊字符1）^表示以某个字符开头&emsp; $ 表示以某个字符结尾&emsp; . 表示一个任意字符&emsp; 表示前面一个字符出现任意多次(可为0次)&emsp; ? 表示非贪婪匹配&emsp;( ) 表示提取字串，通过group( )方法&emsp;+ 表示前面一个字符至少出现一次(注意于 号的区别)&emsp;{ }表示前面一个字符出现的次数 &emsp;e.g. {2}出现2次&emsp; {2,}出现大于等于2次&emsp; {2，5}出现2~5次&emsp;| 表示或关系&emsp;[ ]表示满足中括号内的任意一个字符即可 &emsp;e.g. [0~9]{9}表示有9个字符，每个字符在0~9内 &emsp;&emsp;&emsp; &emsp; [^1]{9}表示有9个字符且每个字符都不为1（这里^表示非关系）&emsp;\s表示空格&emsp;\S表示只要不为空格都能匹配&emsp;\w表示用于匹配字母，数字或下划线字符 &emsp;相当于[a-zA-Z0-9_]&emsp;\W表示用于匹配与\w不匹配的字符&emsp;[\u4E00-\u9FA5]表示一个汉字&emsp;\d表示一个数字 字符串编码 计算机只能处理数字，文本转换为数字才能处理。计算机中8个bit作为一个字节，所以一个字节能表示的最大数字为255 计算机是美国人发明的，因此一个字节可以表示所有字符了，所以ASCII编码就成为美国人的标准编码 但是ASCII编码处理中文是明显不够的，中文不止255个汉字，所以中国制定了GB2312编码，用两个字节表示一个汉字。GB2312还把ASCII编码包含进去了，同理，日文、韩文等都发展了一套字节的编码，标准就越来越多，如果出现多种语言混合显示就一定会出现乱码 于是，unicode出现了，将所有语言统一到一套编码里 看一下ASCII和unicode编码： 1） 字母A用ASCII编码十进制是65，二进制是0100 0001 2） 汉字”中”已超出ASCII编码的范围，用unicode编码是20013，二进制是01001110 00101101 3） A用unicode编码只需要补0 所以二进制编码是 00000000 0100 0001 乱码问题解决了，但是如果内容都是英文，unicode编码比ASCII编码需要多一倍的存储空间，同时传输也需要多一倍的传输 由此出现了可变长编码”utf-8”编码，把英文变成一个字节，汉字3个字节。特别生僻的4-6字节，如果传输大量英文，utf-8的作用就很明显。 计算机处理变长字符串不方便，因此在内存中将utf-8编码encode成unicode编码，当我们处理完文件再将它转换长utf-8编码存储起来 python2和python3一个最大的区别就是python3中都是用unicode统一编码，而python2中中文是用utf-8编码，python中encode方法的对象必须是unicode编码，如：在python2中，encode一个中文字符串需要先decode(“gb2312”)再encode(“utf8” ),而在python3中则可以直接encode(“utf8” )。这也是为什么python2在开头要声明编码为utf-8的原因，否则中文会出现乱码。 深度优先和广度优先 网站的树结构如图，二级url中常常也包含一级url，为了避免陷入重复爬取同一url，我们可以将已爬取的url记录下 来，当要爬取一个url时先检查是否爬取过，这样就可避免陷入死循环。 深度优先算法 广度优先算法 爬虫去重策略 将访问过的url保存到数据库中(基本不用，效率太低) 将访问过的url保存到set中，只需要O(1)的代价就可查询url(url很多时，非常占存储空间)e.g 1个亿的url(假设一个url占50Byte) 100000000 2 50Byte/1024/1024/1024 = 9GB url经过md5等方法哈希后保存到set中(压缩url所占字符数，scrapy即采用这种方法) 用bitmap方法，将访问过的url通过hash函数映射到某一位(进一步降低了存储空间，但是冲突可能会非常高，所以也不太适用) bloomfilter方法对bitmap进行改进，多重hash函数降低冲突e.g 100000000/8/1024/1024 = 12MB xpathxpath简介 xpath使用路径表达式在xml和html中进行导航 xpath包含标准函数库 xpath是一个w3c标准xpath语法参见W3C:XPath 语法 session和cookie http协议是一种无状态的请求，服务器只响应用户请求并返回相应内容 为了产生有状态的请求(即让服务器识别出是哪个用户发起的请求)，就产生了cookie，当我们初次访问一个网站时，它的服务器会给我们返回一个id，这个id就会保存在我们本地的cookie中，当我们下次再访问它时就会带着这个id，服务器就会给我们返回与这个id有关的信息(注意:cookie与我们所访问的域名是绑定的，cookie是不能跨域访问的) cookie中常常会包含我们的一些重要信息，如用户名和密码等，这就产生了安全性的问题，于是服务器就对这些信息进行加密，加密过后的信息就是session，就有相应的session_key、session_data等返回并存储到我们本地的cookie中，所以在本地cookie中我们常常会看到一些字符串，这就是加密过后的我们的一些信息，通常session都会有一个过期时间来增强安全性]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>spider</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy分布式爬虫打造搜索引擎（一）——环境配置]]></title>
    <url>%2F2019%2F04%2F26%2Fscrapy%E5%88%86%E5%B8%83%E5%BC%8F%E7%88%AC%E8%99%AB%E6%89%93%E9%80%A0%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94%E2%80%94%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[开始打造我们的搜索引擎前，先来配置好我们的环境吧！ 开发环境搭建 IDE — pycharm 数据库 — mysql、redis、elasticsearch 开发环境 — virtualenv pycharm的安装和使用 官网安装pycharm(Professional) 使用license server激活pycharm(可参考Jetbrains系列产品2019.1.1最新激活方法) 打开pycharm，在File-Settings下设置熟悉的keymap和相应的interpreter mysql和navicat的安装和使用 官网安装对应系统的mysql (note:记住用户名和密码，后面连接数据库时要用到) 安装navicate，网上破解方法很多，这里不再赘述 命令行下启动mysql mysql -uroot -p 远程访问数据库：linux下修改mysql配置文件 sudo vim /etc/mysql/mysql.conf.d/mysqld.cnf（windows下的配置文件为my.ini）修改其中的bind-address为0.0.0.0(默认为127.0.0.1，即本地) 设置权限,使所有root用户都可以访问数据库,有两条命令 GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39; IDENTIFIED BY &#39;root&#39; WITH GRANT OPTION;和flush privileges; 打开navicat，连接mysql后，新建数据库时注意字符集选择utf8（否则会出现中文乱码），排序规则选择utf8_general_ci 虚拟环境的安装和配置不同项目的环境配置不一样，使用虚拟环境可以很好地解决这个问题。pip install virtualenv /pip install virtualenvwrapper用virualenv和virtualenvwrapper可以很好地进行虚拟环境地配置，具体使用可参考网上教程。]]></content>
      <categories>
        <category>python</category>
      </categories>
      <tags>
        <tag>spider</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[二叉树的遍历(递归和迭代的C++实现)]]></title>
    <url>%2F2019%2F04%2F18%2F%E4%BA%8C%E5%8F%89%E6%A0%91%E7%9A%84%E9%81%8D%E5%8E%86-%E9%80%92%E5%BD%92%E5%92%8C%E8%BF%AD%E4%BB%A3%E7%9A%84C-%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[基于leetcode上的几道题对二叉树的遍历进行复习。 前序遍历(Leetcode 144)先访问根节点，然后遍历左子树，最后遍历右子树。 递归算法12345678910111213141516171819202122232425/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; preorderTraversal(TreeNode* root)&#123; vector&lt;int&gt; res(0); if(root == nullptr) return res; preorderHelper(root,res); return res; &#125; void preorderHelper(TreeNode* root,vector&lt;int&gt; &amp;res)&#123; if(root == nullptr) return; res.push_back(root-&gt;val); preorderHelper(root-&gt;left,res); preorderHelper(root-&gt;right,res); &#125;&#125;; 迭代算法1234567891011121314151617181920212223242526272829/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; preorderTraversal(TreeNode* root) &#123; vector&lt;int&gt; res(0); if(root == nullptr) return res; stack&lt;TreeNode*&gt; tmp; //用栈来模拟递归调用的过程 TreeNode* curNode; tmp.push(root); while(!tmp.empty())&#123; curNode = tmp.top(); res.push_back(curNode-&gt;val); tmp.pop(); // 按右结点、左结点的顺序入栈，这样出栈时就会先访问左子树，再访问右子树 if(curNode-&gt;right != nullptr) tmp.push(curNode-&gt;right); if(curNode-&gt;left != nullptr) tmp.push(curNode-&gt;left); &#125; return res; &#125;&#125;; 中序遍历(Leetcode 94)先遍历左子树，然后访问根节点，最后遍历右子树。 递归算法123456789101112131415161718192021222324/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; inorderTraversal(TreeNode* root)&#123; vector&lt;int&gt; res(0); if(root == nullptr) return res; inorderHelper(root,res); return res; &#125; void inorderHelper(TreeNode* root,vector&lt;int&gt; &amp;res)&#123; if(root == nullptr) return; inorderHelper(root-&gt;left,res); res.push_back(root-&gt;val); inorderHelper(root-&gt;right,res); &#125;&#125;; 迭代算法1234567891011121314151617181920212223242526272829/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; inorderTraversal(TreeNode* root) &#123; vector&lt;int&gt; res; stack&lt;TreeNode*&gt; tmp; while (!tmp.empty() || root != NULL) &#123; if (root != NULL) &#123; tmp.push(root); root = root-&gt;left; //访问最左节点 &#125; else &#123; root = tmp.top(); res.push_back((tmp.top())-&gt;val); tmp.pop(); root = root-&gt;right; &#125; &#125; return res; &#125;&#125;; 后序遍历(Leetcode 145)先遍历左子树，然后遍历右子树，最后访问根结点 递归算法123456789101112131415161718192021222324/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; postorderTraversal(TreeNode* root)&#123; vector&lt;int&gt; res; if(root == nullptr) return res; postorderHelper(root,res); return res; &#125; void postorderHelper(TreeNode* root,vector&lt;int&gt; &amp;res)&#123; if(root == nullptr) return; postorderHelper(root-&gt;left,res); postorderHelper(root-&gt;right,res); res.push_back(root-&gt;val); &#125;&#125;; 迭代算法12345678910111213141516171819202122232425262728293031323334353637/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;int&gt; postorderTraversal(TreeNode* root) &#123; vector&lt;int&gt; res; stack&lt;TreeNode*&gt; tmp; TreeNode* curNode = root; if(curNode) tmp.push(root); else return res; while(!tmp.empty())&#123; curNode = tmp.top(); if(curNode-&gt;left) &#123; tmp.push(curNode-&gt;left); curNode-&gt;left = NULL; //将访问过节点的左子树设为空，否则会进入死循环 &#125; else&#123; if(curNode-&gt;right)&#123; tmp.push(curNode-&gt;right); curNode-&gt;right = NULL; &#125; else&#123; res.push_back(curNode-&gt;val); tmp.pop(); &#125; &#125; &#125; return res; &#125;&#125;; 层序遍历(Leetcode 102)1234567891011121314151617181920212223242526272829303132/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: vector&lt;vector&lt;int&gt;&gt; levelOrder(TreeNode* root) &#123; vector&lt;vector&lt;int&gt;&gt; res; queue&lt;TreeNode*&gt; que; TreeNode* curNode = root; if(curNode) que.push(curNode); else return res; while(!que.empty())&#123; int size = que.size(); //获取该层节点个数 vector&lt;int&gt; tmp; while(size--)&#123; curNode = que.front(); que.pop(); tmp.push_back(curNode-&gt;val); if(curNode-&gt;left) que.push(curNode-&gt;left); if(curNode-&gt;right) que.push(curNode-&gt;right); &#125; res.push_back(tmp); &#125; return res; &#125;&#125;;]]></content>
      <categories>
        <category>algorithm and data structure</category>
      </categories>
      <tags>
        <tag>binary tree</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Exercise On Leetcode]]></title>
    <url>%2F2019%2F04%2F11%2FExercise-On-Leetcode%2F</url>
    <content type="text"><![CDATA[Record my thoughts and solutions to the problems on leetcode. Depth-first SearchEasyleetcode-100思路：判断两颗树相同需两个条件： 当前节点值相同 左子树相同且右子树相同 基于此和树的递归数据结构容易写出递归程序解决该问题。时间复杂度:&emsp;&emsp;O(N) &emsp;//每个节点访问一次空间复杂度:&emsp;&emsp;最好: O(log(N)) (完全平衡二叉树)&emsp;&emsp;最坏: O(N) (退化为链表) &emsp;//N为维护递归栈的深度 代码：c++:123456789101112131415161718192021/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: bool isSameTree(TreeNode* p, TreeNode* q) &#123; if(p==NULL &amp;&amp; q==NULL) return true; if(p==NULL || q==NULL) return false; if(p-&gt;val == q-&gt;val)&#123; return isSameTree(p-&gt;left,q-&gt;left) &amp;&amp; isSameTree(p-&gt;right,q-&gt;right); &#125;else&#123; return false; &#125; &#125;&#125;; python:12345678910111213141516# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def isSameTree(self, p: TreeNode, q: TreeNode) -&gt; bool: if p==None and q==None: return True if p==None or q==None: return False if p.val == q.val: return self.isSameTree(p.right,q.right) and self.isSameTree(p.left,q.left) else: return False leetcode-897思路：考虑到这是一棵二叉搜索树，使用中序遍历的同时将访问的结点连接到结果结点的右子树并将左子树设为空，即可将原二叉树的结点升序输出了。 时间复杂度: O(N) &emsp;//每个节点访问一次空间复杂度: O(H) &emsp;//H为树的高度 代码：c++:1234567891011121314151617181920212223242526272829/** * Definition for a binary tree node. * struct TreeNode &#123; * int val; * TreeNode *left; * TreeNode *right; * TreeNode(int x) : val(x), left(NULL), right(NULL) &#123;&#125; * &#125;; */class Solution &#123;public: TreeNode* increasingBST(TreeNode* root) &#123; if(!root) return NULL; TreeNode dummy(0); p = &amp;dummy; InOrderTravel(root); return dummy.right; &#125; void InOrderTravel(TreeNode* root)&#123; if(!root) return; InOrderTravel(root-&gt;left); p-&gt;right = root; p = root; p-&gt;left = NULL; InOrderTravel(root-&gt;right); &#125;private: TreeNode* p;&#125;; python:1234567891011121314151617181920# Definition for a binary tree node.# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def increasingBST(self, root: TreeNode) -&gt; TreeNode: def inorder(node): if node: inorder(node.left) self.cur.right = node self.cur = node self.cur.left = None inorder(node.right) ans = self.cur = TreeNode(None) inorder(root) return ans.right]]></content>
      <categories>
        <category>algorithm and data structure</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F04%2F07%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
      <categories>
        <category>Blog</category>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
